# Implementing transformers from scratch

In this repo I will be implementing a encoder-decoder transformer architecture from scratch:
The orignal transformer architecture [Attention is All You Need,2017](https://arxiv.org/abs/1706.03762), which develops both encoder and a decoder for English-to-french and English-to-German translation.
The full architecutre is shown below:
<div align="center">
  <img src="./assets/architecture.png" alt="Transformer Architecture" width="600">
</div>

## Components required:
- Dataset
- tokenizers
- encoder model
- decoder model
- Attention mechansims ()

## Some questions with their answers:
- Q1: What was the problem with RNN and how attention mechanism solved it?
